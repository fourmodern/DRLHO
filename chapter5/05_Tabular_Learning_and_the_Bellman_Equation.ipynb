{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05_Tabular_Learning_and_the_Bellman_Equation.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fourmodern/DRLHO/blob/master/chapter5/05_Tabular_Learning_and_the_Bellman_Equation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LycwkD1EC5w8",
        "colab_type": "text"
      },
      "source": [
        "# 05.Â Tabular Learning and the Bellman Equation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyag1rHaFwS-",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "* Value, state, and optimality\n",
        "* The Bellman equation of optimality\n",
        "* Value of action\n",
        "* The value iteration method\n",
        "* Value iteration in practice\n",
        "* Q-learning for FrozenLake"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yll0OjULBzIe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##prepare\n",
        "!pip install tensorboardX > /dev/null 2>&1\n",
        "!pip install -U tensorflow==2.0.0-beta0 > /dev/null 2>&1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBCvTJUF6Eb_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "955105c5-1533-4fe1-a9d5-5f978cc1eb6b"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkrjbTx4GAP7",
        "colab_type": "text"
      },
      "source": [
        "## Value, state, and optimality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2O1e7BUCnxE",
        "colab_type": "text"
      },
      "source": [
        "<p align=\"center\"><img src=\"https://github.com/fourmodern/DRLHO/raw/master/chapter5/img/f1.jpeg\" width=800 /></p>\n",
        "<p align=\"center\"><img src=\"https://github.com/fourmodern/DRLHO/raw/master/chapter5/img/f2.jpeg\" width=800 /></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgweqToaGUZi",
        "colab_type": "text"
      },
      "source": [
        "## The Bellman equation of optimality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8olMP7zswHXs",
        "colab_type": "text"
      },
      "source": [
        "<p align=\"center\"><img src=\"https://github.com/fourmodern/DRLHO/raw/master/chapter5/img/f3.jpeg\" width=800 /></p>\n",
        "<p align=\"center\"><img src=\"https://github.com/fourmodern/DRLHO/raw/master/chapter5/img/f4.jpeg\" width=800 /></p>\n",
        "<p align=\"center\"><img src=\"https://github.com/fourmodern/DRLHO/raw/master/chapter5/img/f5.jpeg\" width=800 /></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uzyW8_SGT8a",
        "colab_type": "text"
      },
      "source": [
        "## Value of action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvCBYX_mxR0C",
        "colab_type": "text"
      },
      "source": [
        "<p align=\"center\"><img src=\"https://github.com/fourmodern/DRLHO/raw/master/chapter5/img/f6.jpeg\" width=600 /></p>\n",
        "<p align=\"center\"><img src=\"https://github.com/fourmodern/DRLHO/raw/master/chapter5/img/f7.jpeg\" width=200 /></p>\n",
        "<p align=\"center\"><img src=\"https://github.com/fourmodern/DRLHO/raw/master/chapter5/img/f8.jpeg\" width=400 /></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Er-vlTuhx5NA",
        "colab_type": "text"
      },
      "source": [
        "<p align=\"center\"><img src=\"https://github.com/fourmodern/DRLHO/raw/master/chapter5/img/f9.jpeg\" width=800 /></p>\n",
        "<p align=\"center\"><img src=\"https://github.com/fourmodern/DRLHO/raw/master/chapter5/img/f10.jpeg\" width=800 /></p>\n",
        "<p align=\"center\"><img src=\"https://github.com/fourmodern/DRLHO/raw/master/chapter5/img/f11.jpeg\" width=800 /></p>\n",
        "<p align=\"center\"><img src=\"https://github.com/fourmodern/DRLHO/raw/master/chapter5/img/f12.jpeg\" width=600 /></p>\n",
        "<p align=\"center\"><img src=\"https://github.com/fourmodern/DRLHO/raw/master/chapter5/img/f13.jpeg\" width=600 /></p>\n",
        "<p align=\"center\"><img src=\"https://github.com/fourmodern/DRLHO/raw/master/chapter5/img/f14.jpeg\" width=600 /></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSJyf2kfGUqS",
        "colab_type": "text"
      },
      "source": [
        "## The value iteration method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sTMNVLI0eUP",
        "colab_type": "text"
      },
      "source": [
        "<p align=\"center\"><img src=\"https://github.com/fourmodern/DRLHO/raw/master/chapter5/img/f16.jpeg\" width=800 /></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAhSW9n_GU4o",
        "colab_type": "text"
      },
      "source": [
        "## Value iteration in practice\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bx-WlvkS7fPY",
        "colab_type": "text"
      },
      "source": [
        "<p align=\"center\"><img src=\"https://github.com/fourmodern/DRLHO/raw/master/chapter5/img/frozen.jpeg\" width=500 /></p>\n",
        "<p align=\"center\"><img src=\"https://github.com/fourmodern/DRLHO/raw/master/chapter5/img/f17.jpeg\" width=800 /></p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVAG0wK30teC",
        "colab_type": "code",
        "outputId": "57fed25e-4a35-43e2-f49a-727040533f77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import tensorboardcolab as tb\n",
        "tbc = tb.TensorBoardColab()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wait for 8 seconds...\n",
            "TensorBoard link:\n",
            "http://3ad68b93.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1snVoAER36Be",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import collections\n",
        "from tensorboardX import SummaryWriter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_C8YlZV36ML",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ENV_NAME = \"FrozenLake-v0\"\n",
        "GAMMA = 0.9\n",
        "TEST_EPISODES = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3HyWzKT36QL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Agent:\n",
        "    def __init__(self):\n",
        "        self.env = gym.make(ENV_NAME)\n",
        "        self.state = self.env.reset()\n",
        "        self.rewards = collections.defaultdict(float)\n",
        "        self.transits = collections.defaultdict(collections.Counter)\n",
        "        self.values = collections.defaultdict(float)\n",
        "\n",
        "    def play_n_random_steps(self, count):\n",
        "        for _ in range(count):\n",
        "            action = self.env.action_space.sample()\n",
        "            new_state, reward, is_done, _ = self.env.step(action)\n",
        "            self.rewards[(self.state, action, new_state)] = reward\n",
        "            self.transits[(self.state, action)][new_state] += 1\n",
        "            self.state = self.env.reset() if is_done else new_state\n",
        "\n",
        "    def calc_action_value(self, state, action):\n",
        "        target_counts = self.transits[(state, action)]\n",
        "        total = sum(target_counts.values())\n",
        "        action_value = 0.0\n",
        "        for tgt_state, count in target_counts.items():\n",
        "            reward = self.rewards[(state, action, tgt_state)]\n",
        "            action_value += (count / total) * (reward + GAMMA * self.values[tgt_state])\n",
        "        return action_value\n",
        "\n",
        "    def select_action(self, state):\n",
        "        best_action, best_value = None, None\n",
        "        for action in range(self.env.action_space.n):\n",
        "            action_value = self.calc_action_value(state, action)\n",
        "            if best_value is None or best_value < action_value:\n",
        "                best_value = action_value\n",
        "                best_action = action\n",
        "        return best_action\n",
        "\n",
        "    def play_episode(self, env):\n",
        "        total_reward = 0.0\n",
        "        state = env.reset()\n",
        "        while True:\n",
        "            action = self.select_action(state)\n",
        "            new_state, reward, is_done, _ = env.step(action)\n",
        "            self.rewards[(state, action, new_state)] = reward\n",
        "            self.transits[(state, action)][new_state] += 1\n",
        "            total_reward += reward\n",
        "            if is_done:\n",
        "                break\n",
        "            state = new_state\n",
        "        return total_reward\n",
        "\n",
        "    def value_iteration(self):\n",
        "        for state in range(self.env.observation_space.n):\n",
        "            state_values = [self.calc_action_value(state, action)\n",
        "                            for action in range(self.env.action_space.n)]\n",
        "            self.values[state] = max(state_values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_9BmKsl36UR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_env = gym.make(ENV_NAME)\n",
        "agent = Agent()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJNGCaww4m8R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "0a480227-4fab-4ef3-a3d8-e3c114839e48"
      },
      "source": [
        "writer = SummaryWriter(comment=\"-v-iteration\")\n",
        "\n",
        "iter_no = 0\n",
        "best_reward = 0.0\n",
        "\n",
        "while True:\n",
        "  \n",
        "  iter_no += 1\n",
        "\n",
        "  agent.play_n_random_steps(100)\n",
        "  agent.value_iteration()\n",
        "\n",
        "  reward = 0.0\n",
        "  \n",
        "  for _ in range(TEST_EPISODES):\n",
        "    reward += agent.play_episode(test_env)\n",
        "  \n",
        "  reward /= TEST_EPISODES\n",
        "  writer.add_scalar(\"reward\", reward, iter_no)\n",
        " \n",
        "  if reward > best_reward:\n",
        "    print(\"Best reward updated %.3f -> %.3f\" % (best_reward, reward))\n",
        "    best_reward = reward\n",
        "  \n",
        "  if reward > 0.80:\n",
        "    print(\"Solved in %d iterations!\" % iter_no)\n",
        "    break\n",
        "\n",
        "writer.close()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best reward updated 0.000 -> 0.700\n",
            "Best reward updated 0.700 -> 0.750\n",
            "Best reward updated 0.750 -> 0.800\n",
            "Best reward updated 0.800 -> 0.850\n",
            "Solved in 56 iterations!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9g9n4vaGVEV",
        "colab_type": "text"
      },
      "source": [
        "## Q-learning for FrozenLake"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5E5jXX0F7uv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import collections\n",
        "from tensorboardX import SummaryWriter\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrRCsRKmGG93",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ENV_NAME = \"FrozenLake-v0\"\n",
        "GAMMA = 0.9\n",
        "TEST_EPISODES = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9ZADvVH7_3b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Agent:\n",
        "    def __init__(self):\n",
        "        self.env = gym.make(ENV_NAME)\n",
        "        #self.env = wrap_env(gym.make(ENV_NAME))\n",
        "        self.state = self.env.reset()\n",
        "        self.rewards = collections.defaultdict(float)\n",
        "        self.transits = collections.defaultdict(collections.Counter)\n",
        "        self.values = collections.defaultdict(float)\n",
        "\n",
        "    def play_n_random_steps(self, count):\n",
        "        for _ in range(count):\n",
        "            action = self.env.action_space.sample()\n",
        "            new_state, reward, is_done, _ = self.env.step(action)\n",
        "            self.rewards[(self.state, action, new_state)] = reward\n",
        "            self.transits[(self.state, action)][new_state] += 1\n",
        "            self.state = self.env.reset() if is_done else new_state\n",
        "\n",
        "    def select_action(self, state):\n",
        "        best_action, best_value = None, None\n",
        "        for action in range(self.env.action_space.n):\n",
        "            action_value = self.values[(state, action)]\n",
        "            if best_value is None or best_value < action_value:\n",
        "                best_value = action_value\n",
        "                best_action = action\n",
        "        return best_action\n",
        "\n",
        "    def play_episode(self, env):\n",
        "        total_reward = 0.0\n",
        "        state = env.reset()\n",
        "        while True:\n",
        "            action = self.select_action(state)\n",
        "            new_state, reward, is_done, _ = env.step(action)\n",
        "            self.rewards[(state, action, new_state)] = reward\n",
        "            self.transits[(state, action)][new_state] += 1\n",
        "            total_reward += reward\n",
        "            if is_done:\n",
        "                break\n",
        "            state = new_state\n",
        "        return total_reward\n",
        "\n",
        "    def value_iteration(self):\n",
        "        for state in range(self.env.observation_space.n):\n",
        "            for action in range(self.env.action_space.n):\n",
        "                action_value = 0.0\n",
        "                target_counts = self.transits[(state, action)]\n",
        "                total = sum(target_counts.values())\n",
        "                for tgt_state, count in target_counts.items():\n",
        "                    reward = self.rewards[(state, action, tgt_state)]\n",
        "                    best_action = self.select_action(tgt_state)\n",
        "                    action_value += (count / total) * (reward + GAMMA * self.values[(tgt_state, best_action)])\n",
        "                self.values[(state, action)] = action_value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZiNBdrZ8HkJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_env = gym.make(ENV_NAME)\n",
        "agent = Agent()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7k7c1jZ8N8i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "e8c8651d-e2a3-411b-af17-0fb20dad82b1"
      },
      "source": [
        "writer = SummaryWriter(comment=\"-q-iteration\")\n",
        "\n",
        "iter_no = 0\n",
        "best_reward = 0.0\n",
        "\n",
        "while True:\n",
        "  \n",
        "  iter_no += 1\n",
        "\n",
        "  agent.play_n_random_steps(100)\n",
        "  agent.value_iteration()\n",
        "\n",
        "  reward = 0.0\n",
        "  \n",
        "  for _ in range(TEST_EPISODES):\n",
        "    reward += agent.play_episode(test_env)\n",
        "  \n",
        "  reward /= TEST_EPISODES\n",
        "  writer.add_scalar(\"reward\", reward, iter_no)\n",
        " \n",
        "  if reward > best_reward:\n",
        "    print(\"Best reward updated %.3f -> %.3f\" % (best_reward, reward))\n",
        "    best_reward = reward\n",
        "  \n",
        "  if reward > 0.80:\n",
        "    print(\"Solved in %d iterations!\" % iter_no)\n",
        "    break\n",
        "\n",
        "writer.close()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best reward updated 0.000 -> 0.100\n",
            "Best reward updated 0.100 -> 0.600\n",
            "Best reward updated 0.600 -> 0.700\n",
            "Best reward updated 0.700 -> 0.850\n",
            "Solved in 36 iterations!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLDpX-Kg8VX4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}